<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Blog on Seth Russell</title>
    <link>/tags/blog/</link>
    <description>Recent content in Blog on Seth Russell</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <copyright>This work is licensed under a Creative Commons Attribution-ShareAlike 4.0 International License.</copyright>
    <lastBuildDate>Mon, 15 Oct 2018 00:00:00 +0000</lastBuildDate>
    
        <atom:link href="/tags/blog/index.xml" rel="self" type="application/rss+xml" />
    
    
    <item>
      <title>Using Google Compute Engine as host for RStudio Server</title>
      <link>/2018/10/15/2018-10-15-using-google-compute-engine-as-host-for-rstudio-server/</link>
      <pubDate>Mon, 15 Oct 2018 00:00:00 +0000</pubDate>
      
      <guid>/2018/10/15/2018-10-15-using-google-compute-engine-as-host-for-rstudio-server/</guid>
      <description>&lt;p&gt;&lt;em&gt;Note:&lt;/em&gt; This post is a work in progress. Need to verify websockets setup - if it works, document here!&lt;/p&gt;

&lt;p&gt;Google Cloud has a wide range of services available. &lt;a href=&#34;https://www.rstudio.com/products/rstudio-server-pro/&#34;&gt;RStudio Server&lt;/a&gt;  (&lt;a href=&#34;https://www.rstudio.com/products/rstudio/download-server/&#34;&gt;download&lt;/a&gt;), is feature comparable to the desktop software &lt;a href=&#34;https://www.rstudio.com/products/rstudio/&#34;&gt;RStudio IDE&lt;/a&gt;. RStudio Server is available under two different licensing models: &amp;ldquo;Open Source Edition: (AGPL v3), and &amp;ldquo;Commercial License.&amp;rdquo;&lt;/p&gt;

&lt;p&gt;In this post, I&amp;rsquo;ll be setting up the Open Source Edition. Total estimated cost per month if the system is running 24x7: $202.68/month. Cost is much less if you only run the system as needed (business hours only would cost about $45/month)&lt;/p&gt;

&lt;p&gt;Steps:&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;Create new GCE

&lt;ol&gt;
&lt;li&gt;Navigate to &lt;a href=&#34;https://console.cloud.google.com&#34;&gt;https://console.cloud.google.com&lt;/a&gt;. If you don&amp;rsquo;t already have an account, you can setup a free account with $300 credits for 365 days.&lt;/li&gt;
&lt;li&gt;From the navigation menu (top left), select &amp;ldquo;Compute Engine&amp;rdquo; (under &amp;lsquo;COMPUTE&amp;rsquo;) -&amp;gt; &amp;ldquo;VM Instances&amp;rdquo;&lt;/li&gt;
&lt;li&gt;Click &amp;ldquo;CREATE INSTANCE&amp;rdquo; at the top of the page&lt;/li&gt;
&lt;li&gt;Name your instance as desired.&lt;/li&gt;
&lt;li&gt;Pcik desired zone. I selected us-central1 (as that zonehas GPU availability should I want that feature in the future)&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Machine type&lt;/strong&gt;: I selected &lt;strong&gt;8 CPU 30GB RAM (n1-standard-8)&lt;/strong&gt; based on estimated resources needed. This host will support 6 Data Scientist; you can always scale this up or down as needed after VM creation.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Boot Disk&lt;/strong&gt;: Click change - select &lt;strong&gt;Ubuntu 18.04 LTS minimal&lt;/strong&gt; - LTS minimal installs very few packages by default, keeping disk usage minimized. Most Linux distributions are free to use, but not all OSes are; some have an hourly charge.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Boot Disk&lt;/strong&gt;: Recommend setting size to 50GB and selecting &lt;strong&gt;Boot Disk Type&lt;/strong&gt; to SSD. While an SSD will cost $8/more per month than standard persistent disk when running 24x7, it also performs much better when using local files.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Firewall&lt;/strong&gt;: Check both boxes to allow HTTP and HTTPS traffic.&lt;/li&gt;
&lt;li&gt;Click &amp;ldquo;Create&amp;rdquo;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Static IP Address setup&lt;/strong&gt;:

&lt;ol&gt;
&lt;li&gt;Once machine is up (takes perhaps 30 seconds), click the machine name&lt;/li&gt;
&lt;li&gt;Scroll down to the &amp;ldquo;Network Interfaces&amp;rdquo; section and click &amp;ldquo;View Details&amp;rdquo; in the &amp;ldquo;Network Details&amp;rdquo; column.&lt;/li&gt;
&lt;li&gt;From the left menu &amp;ldquo;VPC Network&amp;rdquo;, select &amp;ldquo;External IP addresses&amp;rdquo;&lt;/li&gt;
&lt;li&gt;In the &amp;ldquo;Type&amp;rdquo; column, click the drop down to change from &amp;ldquo;Ephemeral&amp;rdquo; to &amp;ldquo;Static&amp;rdquo;. A static ip reservation results in a constant but small ongoing charge.&lt;/li&gt;
&lt;/ol&gt;&lt;/li&gt;
&lt;/ol&gt;&lt;/li&gt;
&lt;li&gt;Basic machine setup: One the VM Instance page, in the &amp;ldquo;Connect&amp;rdquo; column, click &amp;ldquo;SSH&amp;rdquo;. Once the SSH window pops up, run these commands

&lt;ol&gt;
&lt;li&gt;&lt;code&gt;sudo apt-get install htop tmux dialog vim lsof less&lt;/code&gt; This installs a few basic tools that I find to be helpful&lt;/li&gt;
&lt;li&gt;&lt;code&gt;sudo adduser &amp;lt;name&amp;gt;&lt;/code&gt; Add users as appropriate. RStudio Server uses local accounts to control access to the web UI.&lt;/li&gt;
&lt;/ol&gt;&lt;/li&gt;
&lt;li&gt;Install R and RStudio. I selected to install R 3.5.x by following instructions at &lt;a href=&#34;https://cran.rstudio.com/bin/linux/ubuntu/README.html&#34;&gt;https://cran.rstudio.com/bin/linux/ubuntu/README.html&lt;/a&gt;

&lt;ol&gt;
&lt;li&gt;&lt;code&gt;sudo apt-get update&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;&lt;code&gt;sudo apt-get upgrade&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;&lt;code&gt;apt-key adv --keyserver keyserver.ubuntu.com --recv-keys E298A3A825C0D65DFD57CBB651716619E084DAB9&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;add this line to /etc/apt/sources.list: &lt;code&gt;deb https://cloud.r-project.org/bin/linux/ubuntu bionic-cran35/&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;&lt;code&gt;sudo apt-get install r-base r-base-dev gdebi-core&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;&lt;code&gt;wget https://download2.rstudio.org/rstudio-server-1.1.456-amd64.deb&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;&lt;code&gt;sudo gdebi rstudio-server-1.1.456-amd64.deb&lt;/code&gt;&lt;/li&gt;
&lt;/ol&gt;&lt;/li&gt;
&lt;li&gt;Run R and install additional desired packages. &lt;em&gt;Note:&lt;/em&gt; Installing packages as root installs those packages for all uesrs.

&lt;ol&gt;
&lt;li&gt;&lt;code&gt;sudo apt-get install libssl-dev libcurl4-openssl-dev libxml2-dev wget&lt;/code&gt; This fullfills some dependency requirements for desired packages.&lt;/li&gt;
&lt;li&gt;&lt;code&gt;sudo R&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;&lt;code&gt;install.packages(c(&#39;tidyverse&#39;, &#39;bigrquery&#39;, &#39;devtools&#39;))&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;Verify packages have been installed correctly: &lt;code&gt;require(&amp;quot;dplyr&amp;quot;)&lt;/code&gt; and &lt;code&gt;require(&#39;bigrquery&#39;)&lt;/code&gt; etc.&lt;/li&gt;
&lt;/ol&gt;&lt;/li&gt;
&lt;li&gt;Install Nginx and add Let&amp;rsquo;s Encrypt SSL cert &lt;a href=&#34;https://certbot.eff.org/lets-encrypt/ubuntubionic-nginx&#34;&gt;https://certbot.eff.org/lets-encrypt/ubuntubionic-nginx&lt;/a&gt;

&lt;ol&gt;
&lt;li&gt;&lt;code&gt;sudo apt-get install nginx software-properties-common&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;&lt;code&gt;sudo add-apt-repository ppa:certbot/certbot&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;&lt;code&gt;sudo apt-get update&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;&lt;code&gt;sudo apt-get install python-certbot-nginx&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;Create a new nginx configuration file by copying the default configuration:

&lt;ol&gt;
&lt;li&gt;&lt;code&gt;sudo cp /etc/nginx/sites-available/default /etc/nginx/sites-available/rstudio-server&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;Remove the default site from being served, but keep the configuration for reference: &lt;code&gt;sudo rm /etc/nginx/sites-enabled/default&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;Make sure Nginx still works - &lt;code&gt;sudo systemctl restart nginx&lt;/code&gt;&lt;/li&gt;
&lt;/ol&gt;&lt;/li&gt;
&lt;li&gt;&lt;code&gt;sudo certbot --nginx&lt;/code&gt;&lt;/li&gt;
&lt;/ol&gt;&lt;/li&gt;
&lt;li&gt;Setup Nginx to proxy RStudio Server via https port. Follow instructions similar to &lt;a href=&#34;https://www.digitalocean.com/community/tutorials/how-to-configure-nginx-with-ssl-as-a-reverse-proxy-for-jenkins&#34;&gt;https://www.digitalocean.com/community/tutorials/how-to-configure-nginx-with-ssl-as-a-reverse-proxy-for-jenkins&lt;/a&gt;

&lt;ol&gt;
&lt;li&gt;Modify the :443 portion to do the https proxy - see location{} section&lt;/li&gt;
&lt;li&gt;Resulting nginx configuration file should look like: &lt;a href=&#34;https://gist.github.com/magic-lantern/1b5e11c3cf5964b69e8e7824df015c5d&#34;&gt;https://gist.github.com/magic-lantern/1b5e11c3cf5964b69e8e7824df015c5d&lt;/a&gt;&lt;/li&gt;
&lt;/ol&gt;&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;Some URLs for additional information:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;General setup: &lt;a href=&#34;https://towardsdatascience.com/running-jupyter-notebook-in-google-cloud-platform-in-15-min-61e16da34d52&#34;&gt;https://towardsdatascience.com/running-jupyter-notebook-in-google-cloud-platform-in-15-min-61e16da34d52&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Static IP Address: &lt;a href=&#34;https://cloud.google.com/compute/pricing?hl=en_US&amp;amp;_ga=2.8427253.-1719922974.1503442674#ipaddress&#34;&gt;https://cloud.google.com/compute/pricing?hl=en_US&amp;amp;_ga=2.8427253.-1719922974.1503442674#ipaddress&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Network Tier Pricing: &lt;a href=&#34;https://cloud.google.com/network-tiers/pricing&#34;&gt;https://cloud.google.com/network-tiers/pricing&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Nginx as a reverse proxy: &lt;a href=&#34;https://www.digitalocean.com/community/tutorials/how-to-configure-nginx-with-ssl-as-a-reverse-proxy-for-jenkins&#34;&gt;https://www.digitalocean.com/community/tutorials/how-to-configure-nginx-with-ssl-as-a-reverse-proxy-for-jenkins&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Starting and Stoping the RStudio Server Service: &lt;a href=&#34;https://support.rstudio.com/hc/en-us/articles/200532327-Managing-the-Server&#34;&gt;https://support.rstudio.com/hc/en-us/articles/200532327-Managing-the-Server&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;RStudio Server setup using PAM and Apache : &lt;a href=&#34;https://jstaf.github.io/2018/06/20/rstudio-server-semi-pro.html&#34;&gt;https://jstaf.github.io/2018/06/20/rstudio-server-semi-pro.html&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
</description>
    </item>
    
    <item>
      <title>How to profile your R code that calls C/C&#43;&#43; </title>
      <link>/2018/10/05/2018-10-05-how-to-profile-your-r-code-that-calls-c-c-plus-plus/</link>
      <pubDate>Fri, 05 Oct 2018 00:00:00 +0000</pubDate>
      
      <guid>/2018/10/05/2018-10-05-how-to-profile-your-r-code-that-calls-c-c-plus-plus/</guid>
      <description>

&lt;p&gt;Software optimization is a key aspect of software development, especially when working with large datasets. The only way to know if your software will perform adequately under ideal (and non-ideal) circumstances is to use a careful combination of tests and benchmarking/code profiling tools. Code profilers show how a software behaves and what functions are being called while benchmarking tools generally focus on just execution time. Some tools combine both profiling and benchmarking. In R, some of the common tools to help with this are &lt;a href=&#34;https://github.com/joshuaulrich/microbenchmark/&#34;&gt;microbenchmark&lt;/a&gt;, &lt;a href=&#34;http://collectivemedia.github.io/tictoc/&#34;&gt;tictoc&lt;/a&gt;, Rprof (from R utils package), &lt;a href=&#34;https://CRAN.R-project.org/package=proftools&#34;&gt;proftools&lt;/a&gt;, and &lt;a href=&#34;https://rstudio.github.io/profvis/&#34;&gt;profvis&lt;/a&gt;. All of the R based tools have limitations with respect to insight into performance of compiled code: visibility stops at the .Call() function. In order to get around this limitation, the use of a more general purpose profiling tool is required.&lt;/p&gt;

&lt;p&gt;While there are many options for profiling C/C++ (or other compiled code) on Linux, many of those tools are difficult to get working correctly with R. Non-Linux users and developers may not be familiar with tools available on their respective platforms. Furthermore, profiling R code with C++ is complicated due to different settings between various OSes. While there are some blog posts and presentation materials available on the Internet, many miss important steps that take some effort to determine the correct solution to resolve.&lt;/p&gt;

&lt;h2 id=&#34;steps-to-profile-c-code-being-called-by-r-code-using-a-gui&#34;&gt;Steps to profile C++ code being called by R code using a GUI&lt;/h2&gt;

&lt;p&gt;macOS users can use Xcode (freely available) for profiling of R code that calls C++ code. As Xcode has a nice GUI, it may be the preferred tool for many users. The primary profiling tool in Xcode is called &lt;a href=&#34;https://help.apple.com/instruments/mac/10.0/#/dev7b09c84f5&#34;&gt;Instruments&lt;/a&gt;. Instruments can be launched by first opening Xcode, then from the Xcode application menu select Open Developer Tool-&amp;gt;Instruments.&lt;/p&gt;

&lt;p&gt;For users on Windows, the GPL licensed &lt;a href=&#34;http://www.codersnotes.com/sleepy/&#34;&gt;Very Sleepy&lt;/a&gt; is an excellent GUI profiler that works almost identically to Xcode Instruments for CPU Profiling.&lt;/p&gt;

&lt;p&gt;For simplicity sake, steps below demonstrate using Instruments; for other GUIs, steps are very similar. Here are some basic steps to get it working:&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;Configure compilation to enable debugging. In GCC and CLANG this can be accomplished by adding ‘-g’ to your compiler flags via Makevars. If you are developing a package, Makevars or Makevars.win should be in the &amp;ldquo;src&amp;rdquo; directory. Otherwise, update/check ~/.R/Makevars (linux and mac) or .R/Makevars.win (windows) file in your home directory. Depending on your other compiler settings you may also want to add -O0, though changing optimization levels may alter any gains achieved through profiling.&lt;/li&gt;
&lt;li&gt;Determine what you want to profile. &lt;em&gt;The code you run needs to last sufficiently long to allow for application switching and to gather sufficient data via profiling.&lt;/em&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;Create a script to run the code you are interested in. Here is an example of running PCCC with an input dataset of 500000 rows:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;library(pccc)
# replicate sample dataset from pccc package to get larger set
# so that it will run long enough to profile. adjust as needed...
mydata &amp;lt;- pccc::pccc_icd10_dataset
large_dataset &amp;lt;- mydata[rep(seq_len(nrow(mydata)), 500), ]
# function that does work to be profiled
ccc(large_dataset[, c(1:21)], # get id, dx, and pc columns
    id      = id,
    dx_cols = dplyr::starts_with(&amp;quot;dx&amp;quot;),
    pc_cols = dplyr::starts_with(&amp;quot;pc&amp;quot;),
    icdv    = 10)
&lt;/code&gt;&lt;/pre&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;Prep Instruments by selecting ‘Time Profiler’ and then identifying the necessary process. If running script via RStudio, you will need to observe the rsession process. If running via command line R, the process is R. Alternatively run Instruments with “Allocations” to see memory allocations&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;Start profiling.&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;Start your script in whatever fashion you normally run R scripts.&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;After you have gathered sufficient data (perhaps 30 seconds to 1 minute), stop the profiling process.&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;Find your custom function calls in the symbol tree. There will likely be many layers before your code is called. The call just before your code will be “do_dotcall” The symbol tree should show your custom function names and how long each took.&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;Xcode Instruments screenshot showing C++ code from &lt;a href=&#34;https://cran.r-project.org/package=pccc&#34;&gt;PCCC&lt;/a&gt; R package&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;/post/2018-10-05-how-to-profile-your-r-code-that-calls-c-c-plus-plus_files/instruments_screenshot.png&#34; alt=&#34;Xcode Instruments screenshot showing profiling of R code that calls C/C++&#34; /&gt;&lt;/p&gt;

&lt;h2 id=&#34;steps-to-profile-c-code-via-command-line-on-macos-and-linux-using-gperftools&#34;&gt;Steps to profile C++ code via command line on macOS and Linux using gperftools&lt;/h2&gt;

&lt;p&gt;To profile R code that calls C++ code via command line tools requires calling the correct R binary as well as setting up correct environment variables in a OS specific fashion.&lt;/p&gt;

&lt;p&gt;For macOS, I recommend installing gperftools via &lt;a href=&#34;https://brew.sh&#34;&gt;Homebrew&lt;/a&gt; &lt;code&gt;brew install gperftools&lt;/code&gt;. For Linux, use your respective package manager (rpm/apt/pacman/etc.):&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;# Debian derivatives
sudo apt-get install google-perftools
# Redhat derivates
sudo yum install gperftools
&lt;/code&gt;&lt;/pre&gt;

&lt;ol&gt;
&lt;li&gt;Follow steps 1 – 3 as shown above.&lt;/li&gt;

&lt;li&gt;&lt;p&gt;Identify location of R binary. By default ‘R’ is actually a batch shell script that launches the R binary. You cannot get the desired profiling informaiton about your code from profiling a shell script.&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;For macOS, using R installed via Homebrew, the actual R binary is located at &lt;code&gt;/usr/local/Cellar/r/3.5.1/lib/R/bin/exec/R&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;For Linux users, the R binary is likely located at &lt;code&gt;/usr/lib/R/bin/exec/R&lt;/code&gt;&lt;/li&gt;
&lt;/ol&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;Set (or verify) your R_HOME environment variable.&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;For macOS, using R installed via Homebrew, the R_HOME is &lt;code&gt;/usr/local/Cellar/r/3.5.1/lib/R&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;For Linux users, the R binary is likely located at &lt;code&gt;/usr/lib/R/bin/exec/R&lt;/code&gt;&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;If your environment variable is not set, set R_HOME via a command like (replace path with your actual R Home location). If you use bash, the command is:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;export R_HOME=/usr/lib/R/bin/exec/R
&lt;/code&gt;&lt;/pre&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;Run your test (&lt;em&gt;in this example it is called profile_sample.R&lt;/em&gt;) script with the perftools libraries loaded and an environment variable CPUPROFILE that specifies the location to save the CPU profile output. Replace libprofiler path and file name with your actual filename. Replace R binary with your actual R binary with full path.&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;&lt;p&gt;For macOS, this is accomplished via the command &lt;code&gt;DYLD_INSERT_LIBRARIES=/usr/local/lib/libprofiler.dylib CPUPROFILE=sample.profile /usr/local/Cellar/r/3.5.1/lib/R/bin/exec/R -f profile_sample.R&lt;/code&gt;&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;For Linux, this is accomplished via the command &lt;code&gt;LD_PRELOAD=/usr/lib/libprofiler.so CPUPROFILE=sample.profile /usr/lib/R/bin/exec/R -f profile_sample.R&lt;/code&gt;&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;View the results via pprof; again, ensure you use your actual R binary path.&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;For macOS, this is accomplished via the command &lt;code&gt;pprof --text /usr/local/Cellar/r/3.5.1/lib/R/bin/exec/R sample.profile&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;For Linux, this is accomplished via (Debian based distributions may call pprof ‘google-pprof’) the command &lt;code&gt;pprof --text /usr/bin/R ./sample.profile&lt;/code&gt;&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;Output will be something similar to the following:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;Using local file /usr/bin/R.
Using local file ./sample.profile.
Total: 7399 samples
    2252  30.4%  30.4%     2252  30.4% __nss_passwd_lookup
    2172  29.4%  59.8%     4389  59.3% std::__cxx11::basic_string::compare
     982  13.3%  73.1%     5594  75.6% codes::find_match
     591   8.0%  81.1%      621   8.4% Rf_mkCharLenCE
     462   6.2%  87.3%      482   6.5% R_BadLongVector
     223   3.0%  90.3%      223   3.0% std::vector::operator[] (inline)
     151   2.0%  92.4%      151   2.0% std::__once_callable
      98   1.3%  93.7%       98   1.3% SET_STRING_ELT
      83   1.1%  94.8%       83   1.1% _init@6750
      30   0.4%  95.2%      452   6.1% Rf_allocVector3
&lt;/code&gt;&lt;/pre&gt;&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;&lt;em&gt;Updated Oct 6, 2018 based on feedback from Matthew Krump&lt;/em&gt;&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Changed code example to be more easily reproduced&lt;/li&gt;
&lt;li&gt;More details on Makevars&lt;/li&gt;
&lt;li&gt;More details on launching Instruments&lt;/li&gt;
&lt;li&gt;Recommendations on installing gproftools&lt;/li&gt;
&lt;/ul&gt;
</description>
    </item>
    
    <item>
      <title>Example of Python data analysis</title>
      <link>/2018/09/27/2018-09-27-example-of-python-data-analysis/</link>
      <pubDate>Thu, 27 Sep 2018 00:00:00 +0000</pubDate>
      
      <guid>/2018/09/27/2018-09-27-example-of-python-data-analysis/</guid>
      <description>

&lt;p&gt;&lt;em&gt;Work in Progress&amp;hellip;&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;This Analytics example is meant to demonstrate a few of the things you can do to analyze data stored in Google BigQuery using Python. Originally developed some time ago, I thought I&amp;rsquo;d clean it up and post it here.&lt;/p&gt;

&lt;p&gt;For those that aren&amp;rsquo;t familiar with BigQuery, it is a &amp;lsquo;serverless&amp;rsquo; database system that is fully managed and always available. Instead of charging per environment/instance/hour of time like many cloud database systems, Google charges based on the amount of data a query processes. For those familiar with Amazon Web Services, BigQuery is more like DynamoDB rather than an Aurora/Redshift/etc instances where you pay per hour.&lt;/p&gt;

&lt;p&gt;For more details on this example, setup, etc. please see the project wiki: &lt;a href=&#34;https://github.com/CUD2V/analytics_examples/wiki&#34;&gt;https://github.com/CUD2V/analytics_examples/wiki&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;For some additional examples showing more of the capabilities of Google BigQuery or Google Cloud Storage see:
* &lt;a href=&#34;https://gist.github.com/magic-lantern/904e22ca625404da489dab4f2706fdc7&#34;&gt;Google Cloud BigQuery Example&lt;/a&gt; - Some alternative methods of getting data from Google BigQuery.
* &lt;a href=&#34;https://gist.github.com/magic-lantern/c11500847f06a6e63bae4ca010595773&#34;&gt;Google Cloud Storage Example&lt;/a&gt; - Most analysis that a person would want to do will likely include accessing external data or storing results of some process for further downstream analysis. Google Cloud Storage is one option for that phase of an anlysis pipeline.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;import pandas

from IPython.core.display import display, HTML
display(HTML(&amp;quot;&amp;lt;style&amp;gt;.container { width:95% !important; }&amp;lt;/style&amp;gt;&amp;quot;))

import psycopg2 as pg
import pandas.io.sql as psql

from bokeh.io import output_notebook, show
output_notebook()
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# this is to hide useless errors if using OAuth with BigQuery
import logging
logging.getLogger(&#39;googleapiclient.discovery_cache&#39;).setLevel(logging.CRITICAL)
# don&#39;t want to be messaged about future warnings as I&#39;m not explicitly calling code that is being warned about
import warnings
warnings.simplefilter(action=&#39;ignore&#39;, category=FutureWarning)

# set this to either postgres or bigquery ####
datasource = &#39;bigquery&#39;
##############################################

if datasource == &#39;postgres&#39;:
    # get connected to the database
    connection = pg.connect(&amp;quot;host=localhost dbname=ohdsi user=ohdsi password=ohdsi&amp;quot;)

    # print the connection string we will use to connect
    print(&amp;quot;Connecting to database: &amp;quot;, connection)

    # conn.cursor will return a cursor object, you can use this cursor to perform queries
    cursor = connection.cursor()
    print(&amp;quot;Connected to Postgres database!\n&amp;quot;)
elif datasource == &#39;bigquery&#39;:
    connection = {
        &#39;project_id&#39; : &#39;synpuf-omop-project&#39;,
        &#39;dialect&#39;    : &#39;standard&#39;
    }
    print(&amp;quot;Setup Google BigQuery connection&amp;quot;)
else:
    connection = None

def read_data(sql):
    if datasource == &#39;postgres&#39;:
        return pandas.read_sql(sql, connection)
    elif datasource == &#39;bigquery&#39;:
        return pandas.read_gbq(sql, **connection)
    else:
        return pandas.DataFrame()
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&#34;exploration-and-visualization&#34;&gt;Exploration and Visualization&lt;/h2&gt;

&lt;p&gt;&lt;em&gt;These next cells are charts looking at births by year - of the population still active in medicare today.&lt;/em&gt;&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from bokeh.charts import Bar
from bokeh.io import output_notebook, show
from bokeh.charts import defaults
defaults.width = 900
defaults.height = 700

age_df = read_data(&#39;&#39;&#39;
select
    count(year_of_birth) count,
    year_of_birth,
    c1.concept_name gender
from synpuf_omop.person p
left join synpuf_omop.concept c1 on p.gender_concept_id = c1.concept_id
group by gender, year_of_birth
order by year_of_birth, gender
&#39;&#39;&#39;)

p = Bar(age_df,             # source of data
        &#39;year_of_birth&#39;,    # columns from dataframe to use
        #label=&#39;origin&#39;, 
        agg=&#39;sum&#39;,
        values=&#39;count&#39;,
        stack=&#39;gender&#39;,
        title=&amp;quot;Births by year, stacked by gender&amp;quot;,
        legend=&#39;top_right&#39;)
show(p)
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from bokeh.charts import Bar
from bokeh.io import output_notebook, show
from bokeh.charts import defaults
defaults.width = 900
defaults.height = 700

pct_df = read_data(
&#39;&#39;&#39;
select
    year_of_birth,
    count(case when c1.concept_name = &#39;FEMALE&#39; then 1 end) gender_count,
    &#39;FEMALE&#39; gender,
    count(1) total_births
from synpuf_omop.person p
left join synpuf_omop.concept c1 on p.gender_concept_id = c1.concept_id
group by year_of_birth
union all
select
    year_of_birth,
    count(case when c1.concept_name = &#39;MALE&#39; then 1 end) gender_count,
    &#39;MALE&#39; gender,
    count(1) total_births
from synpuf_omop.person p
left join synpuf_omop.concept c1 on p.gender_concept_id = c1.concept_id
group by year_of_birth
order by year_of_birth, gender asc
&#39;&#39;&#39;)

def f(i):
    return float(i[&#39;gender_count&#39;]) / float(i[&#39;total_births&#39;])
pct_df[&#39;pct&#39;] = pct_df.apply(f, axis=1)


p = Bar(pct_df,             # source of data
        values=&#39;pct&#39;,          # y axis
        label=&#39;year_of_birth&#39;, # x axis 
        agg=&#39;sum&#39;,
        stack=&#39;gender&#39;,
        title=&amp;quot;Percentage of births by year, stacked by gender&amp;quot;,
        legend=&#39;top_right&#39;)
show(p)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;em&gt;These next few cells look at drug duration (how long a perscription is to last)&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;Due to differences in SQL dialects, this is the PostgreSQL version - inline below is the Google BigQuery version. Might be possible to make one statement work for both&amp;hellip;&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-sql&#34;&gt;select
    --person_id,
    --drug_concept_id,
    c1.concept_name drug_name,
    --drug_era_start_date,
    --drug_era_end_date,
    drug_era_end_date - drug_era_start_date duration
from synpuf_omop.drug_era d
left join synpuf_omop.concept c1 on d.drug_concept_id = c1.concept_id
where c1.concept_name in (
select drug_name from (
    select
    c1.concept_name drug_name,
    count(1) count
    from synpuf_omop.drug_era d
    left join synpuf_omop.concept c1 on d.drug_concept_id = c1.concept_id
    group by drug_name
    order by count desc
    limit 25
   ) x
)
order by drug_name
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# perhaps modify this query to look at drugs with most variation in duration?

from bokeh.charts import BoxPlot, output_file, show
from bokeh.sampledata.autompg import autompg as df
from bokeh.charts import defaults
defaults.width = 900
defaults.height = 900

dd_df = read_data(
&#39;&#39;&#39;
select
    --person_id,
    --drug_concept_id,
    c1.concept_name drug_name,
    --drug_era_start_date,
    --drug_era_end_date,
    date_diff(cast(drug_era_end_date as date), cast(drug_era_start_date as date), day) as duration
from synpuf_omop.drug_era d
left join synpuf_omop.concept c1 on d.drug_concept_id = c1.concept_id
where c1.concept_name in (
select drug_name from (
    select
    c1.concept_name drug_name,
    count(1) count
    from synpuf_omop.drug_era d
    left join synpuf_omop.concept c1 on d.drug_concept_id = c1.concept_id
    group by drug_name
    order by count desc
    limit 25
   ) x
)
order by drug_name
&#39;&#39;&#39;)

p = BoxPlot(dd_df,
            values=&#39;duration&#39;,      # y axis
            label=&#39;drug_name&#39;,      # x axis
            title=&amp;quot;Drug Duration Box Plot&amp;quot;,
            legend=False,
           )
p.xaxis.axis_label = &amp;quot;Drug&amp;quot;
p.yaxis.axis_label = &amp;quot;Duration (days)&amp;quot;


show(p)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;em&gt;Alternatively, you can just get all the data via more simple SQL SELECT statment and do the data processing via Pandas&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;Again as with the previous query, due to SQL dialect differences, this is the PostgreSQL version - in cell below is the BigQuery version:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-sql&#34;&gt;select
    c1.concept_name drug_name,
    drug_era_end_date - drug_era_start_date duration
from synpuf_omop.drug_era d
left join synpuf_omop.concept c1 on d.drug_concept_id = c1.concept_id
order by drug_name
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;size_df = None
drug_df = None
top_25 = None
top_drugs = None

drug_df = read_data(
&#39;&#39;&#39;
select
    c1.concept_name drug_name,
    date_diff(cast(drug_era_end_date as date), cast(drug_era_start_date as date), day) as duration
from synpuf_omop.drug_era d
left join synpuf_omop.concept c1 on d.drug_concept_id = c1.concept_id
order by drug_name
&#39;&#39;&#39;)

# if we only want to look at 25 most common drugs
# count rows grouping by drug_name
size_df = drug_df.groupby(&amp;quot;drug_name&amp;quot;).size()
# sort the counted result and only return top 25
top_25 = size_df.sort_values(ascending = False).head(25)
# for verification purposes show all rows from original dataset matching most common drug
#drug_df[drug_df.drug_name.str.contains(top_25.index[0]) == True]
# only keep rows that match the top_25 pandas series (single column of a dataframe)
top_drugs = drug_df[drug_df[&#39;drug_name&#39;].isin(top_25.index)]
# for verification sql method says there are 1683795 rows
print(&amp;quot;Does Pandas version match SQL results:&amp;quot;, top_drugs.shape[0] == 1683795)
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from bokeh.charts import BoxPlot, output_file, show
from bokeh.sampledata.autompg import autompg as df
from bokeh.charts import defaults
defaults.width = 900
defaults.height = 900
p = BoxPlot(top_drugs,
            values=&#39;duration&#39;,      # y axis
            label=&#39;drug_name&#39;,      # x axis
            title=&amp;quot;Drug Duration Box Plot&amp;quot;,
            legend=False,
           )
p.xaxis.axis_label = &amp;quot;Drug&amp;quot;
p.yaxis.axis_label = &amp;quot;Duration (days)&amp;quot;


show(p)
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&#34;cohort-identification-prediction&#34;&gt;Cohort identification &amp;amp; Prediction&lt;/h2&gt;

&lt;p&gt;Suppose we want to do some analysis including prediction for patients that complain of lower back pain (SNOMED code 279039007 - see &lt;a href=&#34;http://bioportal.bioontology.org/ontologies/SNOMEDCT?p=classes&amp;amp;conceptid=279039007&#34;&gt;http://bioportal.bioontology.org/ontologies/SNOMEDCT?p=classes&amp;amp;conceptid=279039007&lt;/a&gt; or &lt;a href=&#34;https://phinvads.cdc.gov/vads/http:/phinvads.cdc.gov/vads/ViewCodeSystemConcept.action?oid=2.16.840.1.113883.6.96&amp;amp;code=279039007&#34;&gt;https://phinvads.cdc.gov/vads/http:/phinvads.cdc.gov/vads/ViewCodeSystemConcept.action?oid=2.16.840.1.113883.6.96&amp;amp;code=279039007&lt;/a&gt; for more information)&lt;/p&gt;

&lt;p&gt;The OMOP data model has &lt;a href=&#34;http://www.ohdsi.org/web/wiki/doku.php?id=documentation:cdm:condition_occurrence&#34;&gt;CONDITION_OCCURRENCE&lt;/a&gt; table to document findings. The &lt;a href=&#34;http://www.ohdsi.org/web/wiki/doku.php?id=documentation:cdm:condition_era&#34;&gt;CONDITION_ERA&lt;/a&gt; table is a calculation of a condition duration.&lt;/p&gt;

&lt;p&gt;While there are many elements that we could use for predicting condition duration, suppose we start with basic demographic information about the patient. Here&amp;rsquo;s a query that creates a pandas dataframe with our desired cohort.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;backpain_df = read_data(
&#39;&#39;&#39;
SELECT
  c.person_id,
  gender_concept_id,
  year_of_birth,
  race_concept_id,
  ethnicity_concept_id,
  location_id,
  DATE_DIFF(CAST(condition_era_end_date AS date), CAST(condition_era_start_date AS date), day) AS duration
FROM
  synpuf_omop.condition_era c
LEFT JOIN
  synpuf_omop.person p
ON
  c.person_id = p.person_id
WHERE
  condition_concept_id = 194133
&#39;&#39;&#39;)
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# View summary information about dataset
print(backpain_df.describe().to_string())

# prevent wrapping when printing the full dataframe
pandas.set_option(&#39;display.expand_frame_repr&#39;, False)
print(backpain_df)
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;bf = backpain_df.copy()
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# Most of the data in this dataset is categorical, so need to use dummy encoding on each categorical column
# so that regression will work correctly. Here&#39;s the categorical columns:
#   gender_concept_id     (binary)
#   race_concept_id       (multiple categories)
#   ethnicity_concept_id  (binary)
#   location_id           (multiple categories)
#
# can&#39;t do them all at once - so step through one at a time
bf = pandas.get_dummies(bf, columns=[&#39;gender_concept_id&#39;], drop_first=True)
bf = pandas.get_dummies(bf, columns=[&#39;race_concept_id&#39;], drop_first=True)
bf = pandas.get_dummies(bf, columns=[&#39;ethnicity_concept_id&#39;], drop_first=True)
bf = pandas.get_dummies(bf, columns=[&#39;location_id&#39;], drop_first=True)
print(bf.head())
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# only run this cell if previous results look correct
backpain_df = bf.copy()
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;print(backpain_df.head())
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# divide data into independent and dependent variables

# exclude person_id and duration from independent variables
input_df = backpain_df.drop([&#39;person_id&#39;, &#39;duration&#39;], axis=1)

output_df = backpain_df[&#39;duration&#39;]

# split data into training vs testing dataset
from sklearn.model_selection import train_test_split

input_train, input_test, output_train, output_test = train_test_split(input_df, output_df, test_size = 0.2, random_state = 0)
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# now generate linear regression model on training data
from sklearn.linear_model import LinearRegression
lm = LinearRegression()
lm.fit(input_train, output_train)
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# print intercept and coefficients
print(&amp;quot;Intercept: &amp;quot;, lm.intercept_)
print(&amp;quot;Coefficients: &amp;quot;, lm.coef_)
print(&amp;quot;R^2 value: &amp;quot;, lm.score(input_train, output_train))
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;For a brief explaination of what R&lt;sup&gt;2&lt;/sup&gt; means, see &lt;a href=&#34;http://blog.minitab.com/blog/adventures-in-statistics-2/regression-analysis-how-do-i-interpret-r-squared-and-assess-the-goodness-of-fit&#34;&gt;http://blog.minitab.com/blog/adventures-in-statistics-2/regression-analysis-how-do-i-interpret-r-squared-and-assess-the-goodness-of-fit&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;In general, the closer R&lt;sup&gt;2&lt;/sup&gt; is to 1, the better the model explains variation in data. Plotting or visualizing data along with the predictive model helps to visually understand how close they are to each other.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# now evaluate model on test data

# now predict answers (regression) - since we only care about whole days, round all output to whole numbers
output_pred = pandas.Series(data=lm.predict(input_test))
output_pred = output_pred.round()

for index, value in output_pred.iteritems():
    print(&#39;Real value: &#39;, output_test.values[index], &#39;Predicted value: &#39;, value)
    if index &amp;gt;= 10:
        break
        
# should next calculate various measures such as precision, recall, etc. 
# See https://stackoverflow.com/questions/31421413/how-to-compute-precision-recall-accuracy-and-f1-score-for-the-multiclass-case
# for some examples.
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&#34;final-notes&#34;&gt;Final notes&lt;/h2&gt;

&lt;p&gt;In order to decide which variables are meaningful in the model, methods such as back-propigation, forward-propigation or similar methods should be used.&lt;/p&gt;

&lt;p&gt;Additionally, other regression methods may work better for predicting duration - in fact the data may not even be suited for Linear Regression. In order for Linear Regression to work, certain assumptions about the data must be true. For more information see: &lt;a href=&#34;http://pareonline.net/getvn.asp?n=2&amp;amp;v=8&#34;&gt;http://pareonline.net/getvn.asp?n=2&amp;amp;v=8&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;Also, since this data set has other elements, the inclusion of other factors may help in predicting condition duration.&lt;/p&gt;
</description>
    </item>
    
  </channel>
</rss>